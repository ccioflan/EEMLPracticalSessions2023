{
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.17",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor paralleism in JAX with `pmap`\n",
        "\n",
        "## Setup\n",
        "\n",
        "Public colab TPU instances (https://colab.research.google.com) have an outdated JAX version, as the new version dropped support for colab TPUs.\n",
        "\n",
        "To have access to multiple devices we recommend running it using [Kaggle TPU VMs](https://www.kaggle.com/docs/tpu), which gives you 20 hours of TPU access per week."
      ],
      "metadata": {
        "id": "BIFItcd1PdoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "from typing import Tuple\n",
        "import dataclasses\n",
        "import functools\n",
        "\n",
        "import jax\n",
        "print(jax.__version__)\n",
        "\n",
        "import jax.numpy as jnp\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4DYY4Yyhq8vG",
        "cellView": "form",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:21.272071Z",
          "iopub.execute_input": "2023-07-12T15:49:21.272817Z",
          "iopub.status.idle": "2023-07-12T15:49:21.876766Z",
          "shell.execute_reply.started": "2023-07-12T15:49:21.272775Z",
          "shell.execute_reply": "2023-07-12T15:49:21.875947Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Notebook setting\n",
        "USE_MOCK_DEVICES = True #@param {type:\"boolean\"}\n",
        "import os\n",
        "\n",
        "if USE_MOCK_DEVICES:\n",
        "    print('Using 8 mock devices.')\n",
        "    # Forces XLA to use `n` CPU threads as host devices.\n",
        "    os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
        "\n",
        "    if len(jax.local_devices()) < 8:\n",
        "        raise Exception(\"Notebook requires 8 devices to run\")\n",
        "\n",
        "jax.devices()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7zqgLWSE66Ii",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:21.878351Z",
          "iopub.execute_input": "2023-07-12T15:49:21.878703Z",
          "iopub.status.idle": "2023-07-12T15:49:24.933965Z",
          "shell.execute_reply.started": "2023-07-12T15:49:21.878676Z",
          "shell.execute_reply": "2023-07-12T15:49:24.932959Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a supporting notebook for Introduction to Tensor Parallelism in JAX.\n",
        "\n",
        "It focuses on implementing a simplified version of sharded two-layer, multi-layer perceptron (MLP) from [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf) paper.  Ideally, readers should familiarise themself with [Parallel Evaluation in JAX](https://colab.research.google.com/github/google/jax/blob/main/docs/jax-101/06-parallelism.ipynb) from [JAX 101 series](https://jax.readthedocs.io/en/latest/jax-101/index.html) before working through examples."
      ],
      "metadata": {
        "id": "8e6XsGFrOZyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  MLP"
      ],
      "metadata": {
        "id": "IMrH9YjG1RUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Vanilla transformer $FFN$.\n",
        "We start by detailing the MLP block.\n",
        "\n",
        "In the vanilla transformer we have a fully connected feed-forward newtork, which is applied to each position separately and identically. This consists\n",
        "of two linear transformations with a  $ReLU$ activation:\n",
        "\n",
        "$$\n",
        "FFN(x) = \\max(0, x W_1 \\, + \\, b_1) W_2 + b_2\n",
        "$$\n",
        "\n",
        "\n",
        "$FFN(x) : \\mathbb{R}^{d_{model}} â†’ \\mathbb{R}^{d_{model}}$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "For small models, the dimensionality of the input and output is $d_{\\text{model}}  = 512$, and the inner layer has dimensionality $d_{ff} = 2048$. The actual values are not particularly important. However, it is worth pointing out that $d_{ff} = c \\cdot d_{model}$.\n",
        "\n",
        "Parameters dimensions:\n",
        "* $W_1 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{ff}}$, $b_1 \\in \\mathbb{R}^{d_{ff}}$\n",
        "* $W_2 \\in \\mathbb{R}^{d_{ff} \\times d_{\\text{model}}}$, $b_2 \\in \\mathbb{R}^{d_{\\text{model}}}$\n"
      ],
      "metadata": {
        "id": "4IM6qm4zVW61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To be explicit we define parameters here.\n",
        "@dataclasses.dataclass\n",
        "class Params:\n",
        "    W_1:  jnp.ndarray  # \"* d_model d_ff\"]\n",
        "    W_2:  jnp.ndarray  # \"* d_ff d_model\"]\n",
        "\n",
        "\n",
        "# A vanilla MLP implentatoi\n",
        "def mlp(params: Params, x: jax.Array) -> jax.Array:\n",
        "    \"\"\"Vanila MLP with dropout.\"\"\"\n",
        "    a, b = params\n",
        "    y = jnp.maximum(jnp.matmul(x, a), 0.0)\n",
        "    z = jnp.matmul(y, b)\n",
        "    return z"
      ],
      "metadata": {
        "id": "chDYNSV71PJF",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:24.935168Z",
          "iopub.execute_input": "2023-07-12T15:49:24.935491Z",
          "iopub.status.idle": "2023-07-12T15:49:24.942887Z",
          "shell.execute_reply.started": "2023-07-12T15:49:24.935449Z",
          "shell.execute_reply": "2023-07-12T15:49:24.942034Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Sharded MLP\n",
        "\n",
        "We need to compute $Y = GeLU(XA)$, but both $X$ (data matrix) and $A$ are quite large, and we have a few layers.\n",
        "\n",
        "> One option is to split A along its columns $A = [A_1, A_2 ]$.\n",
        ">\n",
        "> This partitioning allows the GeLU nonlinearity to be independently applied to the output of each partitioned GEMM:\n",
        "> $$ [Y1, \\; Y2] = [GeLU(XA_1), \\; GeLU(XA_2)]  $$\n",
        "\n",
        "\n",
        "\n",
        "<img alt=\"Sharded MLP\" src=\"https://raw.githubusercontent.com/eemlcommunity/PracticalSessions2023/main/tensor_parallelism/mlp.png\" width=\"600px\"/>"
      ],
      "metadata": {
        "id": "oyce8eVqnEcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To set the ground, let's show how a simple 2-layer MLP\n",
        "# would look like in pure JAX.\n",
        "\n",
        "d_model = 4\n",
        "d_ff = 12\n",
        "\n",
        "W_1 = jnp.arange(d_model * d_ff).reshape(d_model, d_ff)\n",
        "b_1 = jnp.arange(d_ff).reshape((1, d_ff))\n",
        "\n",
        "W_2 = jnp.arange(d_model * d_ff).reshape(d_ff, d_model)\n",
        "b_2 = jnp.arange(d_model).reshape((1, d_model))\n",
        "\n",
        "X = jnp.arange(d_model).reshape((1, -1))\n",
        "\n",
        "# Forward pass of a 2-layer vanilla transformer MLP\n",
        "print(jax.nn.relu(X @ W_1 + b_1) @ W_2 + b_2)\n",
        "\n",
        "print(\"\\nTensor shapes:\")\n",
        "for n, t in ((\"W_1\", W_1), (\"b_1\", b_1), (\"W_2\", W_2), (\"b_2\", b_2), (\"X\", X)):\n",
        "    print(n.ljust(2), t.shape, t.dtype)"
      ],
      "metadata": {
        "id": "LylaKO8tnf23",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:24.943975Z",
          "iopub.execute_input": "2023-07-12T15:49:24.944268Z",
          "iopub.status.idle": "2023-07-12T15:49:25.196491Z",
          "shell.execute_reply.started": "2023-07-12T15:49:24.944242Z",
          "shell.execute_reply": "2023-07-12T15:49:25.195446Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation sketches\n",
        "\n",
        "For now we will ignore GeLU and dropout from condisderations."
      ],
      "metadata": {
        "id": "oMDcGi8JNCEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Version without biases\n",
        "A_1, A_2 = jnp.split(W_1, 2, axis=1)  # Split on columns.\n",
        "B_1, B_2 = jnp.split(W_2, 2, axis=0)  # Split on rows.\n",
        "\n",
        "Y_1, Y_2 = [jax.nn.relu(X @ A_1),\n",
        "            jax.nn.relu(X @ A_2)]\n",
        "\n",
        "Z_1, Z_2 = [Y_1 @ B_1,  Y_2 @ B_2]\n",
        "Z = Z_1 + Z_2                         # \"All-reduce \"g\" step\".\n",
        "\n",
        "# Sanity check\n",
        "np.testing.assert_array_equal(\n",
        "    jax.nn.relu(X @ W_1) @ W_2,\n",
        "    Z)"
      ],
      "metadata": {
        "id": "sVFcGzA5HgVq",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:25.199014Z",
          "iopub.execute_input": "2023-07-12T15:49:25.199408Z",
          "iopub.status.idle": "2023-07-12T15:49:25.331651Z",
          "shell.execute_reply.started": "2023-07-12T15:49:25.199379Z",
          "shell.execute_reply": "2023-07-12T15:49:25.330591Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Version with biases\n",
        "A_1, A_2 = jnp.split(W_1, 2, axis=1)  # Split on columns.\n",
        "a_1, a_2 = jnp.split(b_1, 2, axis=1)  # Split on columns.\n",
        "\n",
        "B_1, B_2 = jnp.split(W_2, 2, axis=0)  # Split on rows.\n",
        "\n",
        "Y_1, Y_2 = [jax.nn.relu(X @ A_1 + a_1),  # This happens on device 1\n",
        "            jax.nn.relu(X @ A_2 + a_2)]  # This happens on device 2\n",
        "\n",
        "# For the second bias we need to be a bit smarter,\n",
        "# basically creating [b_2_0, 0, .., 0], [0, b_2_1, ... 0], version of biases,\n",
        "# so that they don't interfere with each other during the all-gather sum collective.\n",
        "\n",
        "## Find indices for the update\n",
        "idx_1, idx_2 = jnp.split(np.arange(d_model), 2)\n",
        "## Split the bias on the \"model\" dimension.\n",
        "B_b1, B_b2 = jnp.split(b_2, 2, axis=1)\n",
        "\n",
        "# Do partial update.\n",
        "Z_1, Z_2 = [(Y_1 @ B_1).at[:,idx_1].add(B_b1),    # This happens on device 1\n",
        "            (Y_2 @ B_2).at[:,idx_2].add(B_b2)]    # This happens on device 2\n",
        "\n",
        "\n",
        "Z = Z_1 + Z_2                                     # \"All-reduce \"g\" step\".\n",
        "\n",
        "# Sanity check\n",
        "np.testing.assert_array_equal(\n",
        "    jax.nn.relu(X @ W_1 + b_1) @ W_2 + b_2 ,\n",
        "    Z)"
      ],
      "metadata": {
        "id": "4iFUadYMIY8P",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:25.332779Z",
          "iopub.execute_input": "2023-07-12T15:49:25.333065Z",
          "iopub.status.idle": "2023-07-12T15:49:25.564342Z",
          "shell.execute_reply.started": "2023-07-12T15:49:25.333040Z",
          "shell.execute_reply": "2023-07-12T15:49:25.563206Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matmul from slides"
      ],
      "metadata": {
        "id": "AnT_AiP3EsC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sharding utils\n",
        "def shard_on_columns(x: jnp.ndarray, N: int) -> jax.Array:\n",
        "    \"\"\"Splits matrix on column axis by number of shards.\"\"\"\n",
        "    assert len(x.shape) == 2\n",
        "    x = jnp.expand_dims(x, axis=0)  # [1, H_1, H_2]\n",
        "    x = jnp.split(x, N, axis=2)     # [1, H_1, H_2/N] N times\n",
        "    x = np.concatenate(x, axis=0)   # [N, H_1, H_2/N]\n",
        "    return x\n",
        "\n",
        "\n",
        "def shard_on_rows(x: jnp.ndarray, N: int) -> jax.Array:\n",
        "    \"\"\"Splits matrix on column axis by number of shards.\"\"\"\n",
        "    assert len(x.shape) == 2\n",
        "    x = jnp.expand_dims(x, axis=0)  # [1, H_1,   H_2]\n",
        "    x = jnp.split(x, N, axis=1)     # [1, H_1/N, H_2] N times\n",
        "    x = np.concatenate(x, axis=0)   # [N, H_1,   H_2/N]\n",
        "    return x\n",
        "\n",
        "\n",
        "def _unshard(x: jnp.ndarray, N: int, axis: int) -> jax.Array:\n",
        "    return jnp.squeeze(jnp.concatenate(jnp.split(x, N, axis=0), axis=axis))\n",
        "\n",
        "\n",
        "def unshard_on_columns(x: jnp.ndarray,  N: int)-> jnp.ndarray:\n",
        "    # `x` should have shape N R C/N\"\n",
        "    assert len(x.shape) == 3\n",
        "    n, r, c_by_n = x.shape\n",
        "    x_unsharded = _unshard(x, N, axis=2)\n",
        "    assert x_unsharded.shape == (r, c_by_n * n)\n",
        "    return x_unsharded\n",
        "\n",
        "\n",
        "def unshard_on_rows(x: jnp.ndarray, N: int) -> jnp.ndarray:\n",
        "    assert len(x.shape) == 3\n",
        "    n, r_by_n, c = x.shape\n",
        "    x_unsharded = _unshard(x, N, axis=1)\n",
        "    assert x_unsharded.shape == (r_by_n * n, c)\n",
        "    return x_unsharded\n",
        "\n",
        "\n",
        "X = np.random.normal(size=(16, 32))\n",
        "np.testing.assert_allclose(X, unshard_on_columns(shard_on_columns(X, 8), 8))\n",
        "np.testing.assert_allclose(X, unshard_on_rows(shard_on_rows(X, 8), 8))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hnxjebBdEXlF",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:25.565478Z",
          "iopub.execute_input": "2023-07-12T15:49:25.565755Z",
          "iopub.status.idle": "2023-07-12T15:49:26.177722Z",
          "shell.execute_reply.started": "2023-07-12T15:49:25.565725Z",
          "shell.execute_reply": "2023-07-12T15:49:26.176514Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim_in, dim_out = (1024, 1024*16)\n",
        "\n",
        "A = jax.random.uniform(jax.random.PRNGKey(0), (dim_in, dim_out))\n",
        "B = jax.random.uniform(jax.random.PRNGKey(1), (dim_out, dim_in))\n",
        "\n",
        "# Manualy reshape data\n",
        "A_pmap = shard_on_columns(A, 8)\n",
        "B_pmap = shard_on_rows(B, 8)\n",
        "\n",
        "def dot_psum(x, y):\n",
        "    return jax.lax.psum(x @ y, 'pmap_axis')"
      ],
      "metadata": {
        "id": "HDPa6j8wElwB",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:26.178871Z",
          "iopub.execute_input": "2023-07-12T15:49:26.179152Z",
          "iopub.status.idle": "2023-07-12T15:49:27.760621Z",
          "shell.execute_reply.started": "2023-07-12T15:49:26.179127Z",
          "shell.execute_reply": "2023-07-12T15:49:27.759511Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_pmap = jax.pmap(dot_psum, axis_name=\"pmap_axis\")\n",
        "C_pmap = dot_pmap(A_pmap, B_pmap)\n",
        "C_host = A @ B\n",
        "\n",
        "np.testing.assert_allclose(\n",
        "    C_pmap[0],  # <- C will be replicated on all N devices, pick any of them.\n",
        "    C_host,\n",
        "    rtol=1e-6   # I can't stress it enough, floats are not your friends.\n",
        ")"
      ],
      "metadata": {
        "id": "Dso1BvfRE3a6",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:27.761949Z",
          "iopub.execute_input": "2023-07-12T15:49:27.762267Z",
          "iopub.status.idle": "2023-07-12T15:49:29.412900Z",
          "shell.execute_reply.started": "2023-07-12T15:49:27.762237Z",
          "shell.execute_reply": "2023-07-12T15:49:29.411751Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "for i in range(25):\n",
        "    C_host = A @ B"
      ],
      "metadata": {
        "id": "VrozTbO8K_Ow",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:29.414052Z",
          "iopub.execute_input": "2023-07-12T15:49:29.414357Z",
          "iopub.status.idle": "2023-07-12T15:49:41.364905Z",
          "shell.execute_reply.started": "2023-07-12T15:49:29.414331Z",
          "shell.execute_reply": "2023-07-12T15:49:41.363534Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@functools.partial(jax.pmap, axis_name=\"pmap_axis\")\n",
        "def loop(A, B):\n",
        "    for i in range(25):\n",
        "        C_pmap = dot_psum(A, B)\n",
        "    return C_pmap"
      ],
      "metadata": {
        "id": "pbbC-6KwK3ZD",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:41.366301Z",
          "iopub.execute_input": "2023-07-12T15:49:41.366638Z",
          "iopub.status.idle": "2023-07-12T15:49:41.372627Z",
          "shell.execute_reply.started": "2023-07-12T15:49:41.366611Z",
          "shell.execute_reply": "2023-07-12T15:49:41.371546Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "C_pmap = loop(A_pmap, B_pmap)"
      ],
      "metadata": {
        "id": "0ej3jx1fLyrh",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:41.373951Z",
          "iopub.execute_input": "2023-07-12T15:49:41.374248Z",
          "iopub.status.idle": "2023-07-12T15:49:41.931753Z",
          "shell.execute_reply.started": "2023-07-12T15:49:41.374222Z",
          "shell.execute_reply": "2023-07-12T15:49:41.930211Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `pmap` Implementation"
      ],
      "metadata": {
        "id": "DYF6_3enGaPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def init_params(nrows: int, ncols: int, dtype = np.float32):\n",
        "    \"\"\"A simple init function.\"\"\"\n",
        "    return lambda rng: jax.random.uniform(rng, shape=(nrows, ncols), dtype=dtype)\n",
        "\n",
        "\n",
        "def make_sharded_mlp(hidden_dim: int,\n",
        "                     pmap_axis='pmap_axis'):\n",
        "    \"\"\"Create Megatron-style sharded MLP, ref: https://arxiv.org/abs/1909.08053.\n",
        "\n",
        "    Implementation notes:\n",
        "    ----\n",
        "    Each Megatron layer consists of two sub-layers:\n",
        "    1) Y = a(X * A), where a is e.g. GeLU, or any other element-wise function.\n",
        "    2) Z = d(Y * B), where d is dropout, or any other element-wise function.\n",
        "\n",
        "    Z = Dropout(GeLU(X @ A) @ B)\n",
        "\n",
        "    We shard A across its columns, and B across its rows:\n",
        "    - X has shape   N x M,\n",
        "    - A has shape   M x C_A and its shard A_i has shape ~ M x (C_A / n)\n",
        "    - B has shape C_A x C_B and its shard B_j has shape ~ (C_A / n) x C_B\n",
        "\n",
        "    For `n` devices, we have:\n",
        "\n",
        "    Z = d(Y @ B) = d([Y_1, ..., Y_n] @ [B_1, ..., B_n]^T)\n",
        "    = d([      Y_1 @ B_1 + ... +       Y_n @ B_n ])\n",
        "    = d([ a(X*A_1) @ B_1 + ... +  a(X*A_n) @ B_n ])\n",
        "             ^                             ^\n",
        "             |                             |\n",
        "         Z_1 on device 1               Z_2 on device 2\n",
        "         \\---------------- all reduce ------------/\n",
        "\n",
        "\n",
        "    where Y_i ~ N x (C_A / n)\n",
        "        Z_i ~ N x  C_B\n",
        "\n",
        "    Note: Only after the all-reduce (psum) step, we are allowed to apply d.\n",
        "\n",
        "    Docs:\n",
        "    ----\n",
        "\n",
        "    The returned `apply_fn` must be wrapped inside of a `jax.pmap` whose axis\n",
        "    name equals `pmap_axis`. The arguments passed to `apply_fn` are assumed to\n",
        "    be sharded.\n",
        "\n",
        "    When creating parameters via the returned `init_fn`, the parameters will be\n",
        "    sharded across devices according to the input shape (the first dimension is\n",
        "    assumed to be sharded).\n",
        "\n",
        "    Args:\n",
        "    hidden_dim: The per-shard hidden dimension. The effective total hidden\n",
        "    dimension is this multiplied by number of shards.\n",
        "    pmap_axis: The sharding axis of the outer pmap.\n",
        "    Returns:\n",
        "    An init_fn and an apply_fn, as per the Stax API conventions.\n",
        "    \"\"\"\n",
        "\n",
        "    def init_fn(rng: jax.random.PRNGKey, input_shape: Tuple[int, int, int]):\n",
        "        num_shards, input_dim, output_dim = input_shape\n",
        "\n",
        "        key_a, key_b = jax.random.split(rng, 2)\n",
        "\n",
        "        keys_a = jax.random.split(key_a, num=num_shards)\n",
        "        param_a = jax.pmap(init_params(input_dim, hidden_dim), 'pmap_axis')(keys_a)\n",
        "\n",
        "        keys_b = jax.random.split(key_b, num=num_shards)\n",
        "        param_b = jax.pmap(init_params(hidden_dim, output_dim), 'pmap_axis')(keys_b)\n",
        "\n",
        "        return input_shape, (param_a, param_b)\n",
        "\n",
        "    def apply_fn(params: Params, x: jax.Array) -> jax.Array:\n",
        "        # Each layer's parameters are sharded accross N devices.\n",
        "        a, b = params\n",
        "        y = jnp.maximum(jnp.matmul(x, a), 0.0)\n",
        "        z = jnp.matmul(y, b)\n",
        "        ...  # All-reduce sum (i.e. sum and broadcast).\n",
        "        return z\n",
        "\n",
        "    return init_fn, apply_fn"
      ],
      "metadata": {
        "id": "Y4kCC_6oGYp-",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:41.933996Z",
          "iopub.execute_input": "2023-07-12T15:49:41.934408Z",
          "iopub.status.idle": "2023-07-12T15:49:41.949812Z",
          "shell.execute_reply.started": "2023-07-12T15:49:41.934375Z",
          "shell.execute_reply": "2023-07-12T15:49:41.948696Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(params: Params, inputs: jax.Array,  targets: jax.Array, logits_fn):\n",
        "    logits = logits_fn(params, inputs)\n",
        "    logits = jax.nn.log_softmax(logits, axis=-1)\n",
        "    # Per batch example.\n",
        "    loss = -jnp.sum(logits * targets, axis=-1)\n",
        "\n",
        "    # Mean loss per example.\n",
        "    return np.mean(loss)"
      ],
      "metadata": {
        "id": "V1q1Aw_DGjBf",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:41.955519Z",
          "iopub.execute_input": "2023-07-12T15:49:41.955990Z",
          "iopub.status.idle": "2023-07-12T15:49:41.963505Z",
          "shell.execute_reply.started": "2023-07-12T15:49:41.955958Z",
          "shell.execute_reply": "2023-07-12T15:49:41.962445Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On TPU donut (2x2) we have 8 cores.\n",
        "num_shards = jax.device_count()\n",
        "\n",
        "batch_size = 2\n",
        "x_dim = 10\n",
        "hidden_dim = 240  # hidden size of the MLP\n",
        "out_dim = 10      # output size of the MLP\n",
        "\n",
        "# Dummy input data with batch size = 2\n",
        "x = jnp.arange(x_dim * batch_size).reshape(batch_size, x_dim)\n",
        "y = jax.nn.one_hot(jnp.arange(1, batch_size + 1), out_dim)\n",
        "\n",
        "# We broadcast* the data.\n",
        "x_b = jax.lax.broadcast(x, (num_shards,))\n",
        "y_b = jax.lax.broadcast(y, (num_shards,))\n",
        "\n",
        "# *) Or to day differntly, we replicate it on different devices."
      ],
      "metadata": {
        "id": "FhJtwZIBGltS",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:41.964690Z",
          "iopub.execute_input": "2023-07-12T15:49:41.964988Z",
          "iopub.status.idle": "2023-07-12T15:49:42.067512Z",
          "shell.execute_reply.started": "2023-07-12T15:49:41.964964Z",
          "shell.execute_reply": "2023-07-12T15:49:42.066259Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = jax.random.PRNGKey(0)\n",
        "init_fn, apply_fn = make_sharded_mlp(1, hidden_dim // num_shards)"
      ],
      "metadata": {
        "id": "Pl6al0C-GnwS",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:42.068923Z",
          "iopub.execute_input": "2023-07-12T15:49:42.069231Z",
          "iopub.status.idle": "2023-07-12T15:49:42.076335Z",
          "shell.execute_reply.started": "2023-07-12T15:49:42.069205Z",
          "shell.execute_reply": "2023-07-12T15:49:42.075293Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap sharded apply_fn with pmap\n",
        "mlp_sharded =  (\n",
        "    lambda params, inputs: jax.pmap(apply_fn, axis_name='pmap_axis')(params, inputs)\n",
        ")"
      ],
      "metadata": {
        "id": "xI31kim5HhlH",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:42.077541Z",
          "iopub.execute_input": "2023-07-12T15:49:42.077812Z",
          "iopub.status.idle": "2023-07-12T15:49:42.084718Z",
          "shell.execute_reply.started": "2023-07-12T15:49:42.077786Z",
          "shell.execute_reply": "2023-07-12T15:49:42.083730Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check if the sharded version is close to the unsharded.\n",
        "hidden_dim = num_shards*2048\n",
        "\n",
        "# Create weights on device\n",
        "W_1_single = np.random.normal(size=(x_dim, hidden_dim))\n",
        "W_2_single = np.random.normal(size=(hidden_dim, x_dim))\n",
        "print(W_1_single.shape, W_2_single.shape)\n",
        "\n",
        "# 'Shard' the data manally for pmap.\n",
        "W_1_sharded = shard_on_columns(W_1_single, num_shards)\n",
        "W_2_sharded = shard_on_rows(W_2_single, num_shards)\n",
        "print(W_1_sharded.shape, W_2_sharded.shape)\n",
        "\n",
        "np.testing.assert_allclose(\n",
        "    mlp_sharded((W_1_sharded, W_2_sharded), x_b)[0],\n",
        "    mlp((W_1_single, W_2_single), x_b[0]),\n",
        "    atol=1e-5, rtol=1e-3)"
      ],
      "metadata": {
        "id": "58n8qzhv1qZj",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:42.085772Z",
          "iopub.execute_input": "2023-07-12T15:49:42.086043Z",
          "iopub.status.idle": "2023-07-12T15:49:42.765950Z",
          "shell.execute_reply.started": "2023-07-12T15:49:42.086018Z",
          "shell.execute_reply": "2023-07-12T15:49:42.764534Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_fn((W_1_single, W_2_single), x_b[0], y_b[0], mlp))\n",
        "print(loss_fn((W_1_sharded, W_2_sharded), x_b, y_b, mlp_sharded))"
      ],
      "metadata": {
        "id": "Wo0HXdws1X58",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:42.767372Z",
          "iopub.execute_input": "2023-07-12T15:49:42.767734Z",
          "iopub.status.idle": "2023-07-12T15:49:43.116089Z",
          "shell.execute_reply.started": "2023-07-12T15:49:42.767703Z",
          "shell.execute_reply": "2023-07-12T15:49:43.114757Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare gradients\n",
        "## Single device MLP.\n",
        "grads_single = jax.grad(functools.partial(loss_fn, logits_fn=mlp))(\n",
        "    (W_1_single, W_2_single), x_b[0], y_b[0])\n",
        "\n",
        "dw1_single, dw2_single = grads_single\n",
        "\n",
        "## Sharded-MLP on 8 devices.\n",
        "grads_sharded = jax.grad(functools.partial(loss_fn, logits_fn=mlp_sharded))(\n",
        "    (W_1_sharded, W_2_sharded), x_b, y_b)\n",
        "\n",
        "dw1_sharded, dw2_sharded = grads_sharded"
      ],
      "metadata": {
        "id": "O5uMqQ4l75tj",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:43.117419Z",
          "iopub.execute_input": "2023-07-12T15:49:43.117739Z",
          "iopub.status.idle": "2023-07-12T15:49:44.197277Z",
          "shell.execute_reply.started": "2023-07-12T15:49:43.117711Z",
          "shell.execute_reply": "2023-07-12T15:49:44.195775Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare gradient norms.\n",
        "print(np.linalg.norm(dw1_sharded))\n",
        "print(np.linalg.norm(dw1_single))"
      ],
      "metadata": {
        "id": "xFpQdWOk8Cqn",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:44.198763Z",
          "iopub.execute_input": "2023-07-12T15:49:44.199078Z",
          "iopub.status.idle": "2023-07-12T15:49:44.207402Z",
          "shell.execute_reply.started": "2023-07-12T15:49:44.199051Z",
          "shell.execute_reply": "2023-07-12T15:49:44.206180Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare running times: 2-layer MLP sharded on 8 devices.\n",
        "%timeit mlp_sharded((W_1_sharded, W_2_sharded), x_b)[0]"
      ],
      "metadata": {
        "id": "9vC7qs3QxF33",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:44.208751Z",
          "iopub.execute_input": "2023-07-12T15:49:44.209065Z",
          "iopub.status.idle": "2023-07-12T15:49:46.007116Z",
          "shell.execute_reply.started": "2023-07-12T15:49:44.209036Z",
          "shell.execute_reply": "2023-07-12T15:49:46.005816Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare running times: 2-layer MLP on 1 device.\n",
        "%timeit mlp((W_1_single, W_2_single), x_b[0])"
      ],
      "metadata": {
        "id": "9qWuUYdhzaK5",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:46.008572Z",
          "iopub.execute_input": "2023-07-12T15:49:46.008904Z",
          "iopub.status.idle": "2023-07-12T15:49:47.939153Z",
          "shell.execute_reply.started": "2023-07-12T15:49:46.008875Z",
          "shell.execute_reply": "2023-07-12T15:49:47.937805Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_with_mlp_sharded = functools.partial(loss_fn, logits_fn=mlp_sharded)\n",
        "\n",
        "\n",
        "def update_pmap_inside(params, x_b, y_b):\n",
        "    loss_val, grads = jax.value_and_grad(loss_with_mlp_sharded)(params, x_b, y_b)\n",
        "    (A, B), (dA, dB) = params, grads\n",
        "\n",
        "    A_new = jax.pmap(lambda x, dx: x - 0.01 * dx, axis_name = 'pmap_axis')(A, dA)\n",
        "    B_new = jax.pmap(lambda x, dx: x - 0.01 * dx, axis_name = 'pmap_axis')(B, dB)\n",
        "\n",
        "    return loss_val, grads, (A_new, B_new)"
      ],
      "metadata": {
        "id": "9DNPAKyOIlLk",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:47.940483Z",
          "iopub.execute_input": "2023-07-12T15:49:47.940789Z",
          "iopub.status.idle": "2023-07-12T15:49:47.948722Z",
          "shell.execute_reply.started": "2023-07-12T15:49:47.940762Z",
          "shell.execute_reply": "2023-07-12T15:49:47.947711Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Can you guess, what's wrong with this implementation?\n",
        "\n",
        "# Can it be improved?\n",
        "input_shape, params = init_fn(key, (num_shards, x_dim, out_dim))\n",
        "print(loss_with_mlp_sharded(params, x_b, y_b))\n",
        "\n",
        "# Warmstart jitted function and print statistics.\n",
        "_, grads, _ = update_pmap_inside(params, x_b, y_b)\n",
        "\n",
        "dw1, dw2 = grads\n",
        "print(f\"Norm of dW_1 %.2f\" % np.linalg.norm(dw1))\n",
        "print(f\"Norm of dW_2 %.2f\\n\" % np.linalg.norm(dw2))\n",
        "\n",
        "for i in range(0, 100):\n",
        "    if i < 10 or i % 10 == 0:\n",
        "        print(i, loss_with_mlp_sharded(params, x_b, y_b))\n",
        "        loss_val, grads, params = update_pmap_inside(params, x_b, y_b)"
      ],
      "metadata": {
        "id": "6oC0mo_hwEHt",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:47.949989Z",
          "iopub.execute_input": "2023-07-12T15:49:47.950284Z",
          "iopub.status.idle": "2023-07-12T15:49:50.406403Z",
          "shell.execute_reply.started": "2023-07-12T15:49:47.950258Z",
          "shell.execute_reply": "2023-07-12T15:49:50.405043Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure the performance.\n",
        "%timeit _, params = init_fn(key, (num_shards, x_dim, out_dim));\n",
        "loss_val, grads, params = update_pmap_inside(params, x_b, y_b)"
      ],
      "metadata": {
        "id": "LHRWJUp-4HJd",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:50.407745Z",
          "iopub.execute_input": "2023-07-12T15:49:50.408057Z",
          "iopub.status.idle": "2023-07-12T15:49:53.189248Z",
          "shell.execute_reply.started": "2023-07-12T15:49:50.408029Z",
          "shell.execute_reply": "2023-07-12T15:49:53.187852Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_from_apply_fn(params, xs, ys):\n",
        "    return loss_fn(params, xs, ys, apply_fn)\n",
        "\n",
        "@functools.partial(jax.pmap, axis_name='pmap_axis')\n",
        "def update_pmap_outside(params, xs, ys):\n",
        "    loss_val, grads = jax.value_and_grad(loss_from_apply_fn)(params, xs, ys)\n",
        "    new_params = jax.tree_map(\n",
        "      lambda param, g: param - g * 0.01, params, grads)\n",
        "\n",
        "    return loss_val, grads, new_params"
      ],
      "metadata": {
        "id": "5347bH-VGSkB",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:53.190649Z",
          "iopub.execute_input": "2023-07-12T15:49:53.190974Z",
          "iopub.status.idle": "2023-07-12T15:49:53.198715Z",
          "shell.execute_reply.started": "2023-07-12T15:49:53.190940Z",
          "shell.execute_reply": "2023-07-12T15:49:53.197568Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Can it be improved?\n",
        "_, params = init_fn(key, (num_shards, x_dim, out_dim))\n",
        "\n",
        "# Warmstart jitted function and print statistics.\n",
        "loss_val, grads, _ =  update_pmap_outside(params, x_b, y_b)\n",
        "\n",
        "print(loss_val.shape)\n",
        "print(loss_val[0])\n",
        "\n",
        "dw1, dw2 = grads\n",
        "print(f\"Norm of dW_1 %.2f\" % np.linalg.norm(dw1))\n",
        "print(f\"Norm of dW_2 %.2f\\n\" % np.linalg.norm(dw2))"
      ],
      "metadata": {
        "id": "gTOTiEuR5kO1",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:53.199846Z",
          "iopub.execute_input": "2023-07-12T15:49:53.200124Z",
          "iopub.status.idle": "2023-07-12T15:49:53.749402Z",
          "shell.execute_reply.started": "2023-07-12T15:49:53.200098Z",
          "shell.execute_reply": "2023-07-12T15:49:53.747992Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit _, params = init_fn(key, (num_shards, x_dim, out_dim));\n",
        "loss_val, grads, params = update_pmap_outside(params, x_b, y_b)"
      ],
      "metadata": {
        "id": "AyGtuwGxJp8m",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:49:53.750742Z",
          "iopub.execute_input": "2023-07-12T15:49:53.751054Z",
          "iopub.status.idle": "2023-07-12T15:50:12.478425Z",
          "shell.execute_reply.started": "2023-07-12T15:49:53.751026Z",
          "shell.execute_reply": "2023-07-12T15:50:12.476993Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Note about gradients"
      ],
      "metadata": {
        "id": "LdUuTnWdJbgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a refresher the partial derivatives of $f: \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R} $\n",
        "$$\n",
        "f(x,y) = x + y \\hspace{0.5in} \\rightarrow \\hspace{0.5in} \\frac{\\partial f}{\\partial x} = 1 \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = 1\n",
        "$$\n",
        "\n",
        "in the context of AD it's straightforward to see, that sum operation will distribute gradients equally to all its inputs during backproagation."
      ],
      "metadata": {
        "id": "AmdDznzRPQWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.array(1.0, dtype=jnp.float32)\n",
        "y = jnp.array(1.0, dtype=jnp.float32)\n",
        "\n",
        "z = jnp.ones_like(x) * 5\n",
        "\n",
        "jax.make_jaxpr(jax.grad(lambda x, y: x + y))(x, y)"
      ],
      "metadata": {
        "id": "0ZzRNOJtQ9sU",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:50:12.479904Z",
          "iopub.execute_input": "2023-07-12T15:50:12.480234Z",
          "iopub.status.idle": "2023-07-12T15:50:12.522829Z",
          "shell.execute_reply.started": "2023-07-12T15:50:12.480205Z",
          "shell.execute_reply": "2023-07-12T15:50:12.521713Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is evident in the vjp of sum, it just redistributes the incoming signal\n",
        "# backwards to the input.\n",
        "jax.make_jaxpr(jax.vjp(lambda x, y:  x + y, *(x, y))[1])(z)"
      ],
      "metadata": {
        "id": "vZKag6r7WZPm",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:50:12.524221Z",
          "iopub.execute_input": "2023-07-12T15:50:12.524567Z",
          "iopub.status.idle": "2023-07-12T15:50:12.555381Z",
          "shell.execute_reply.started": "2023-07-12T15:50:12.524537Z",
          "shell.execute_reply": "2023-07-12T15:50:12.554242Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's investigate the first case,\n",
        "# pmap inside of gradient.\n",
        "N_DEVICES = jax.local_device_count()\n",
        "\n",
        "def f(x):\n",
        "  # Just call all-gather from all devices.\n",
        "  return jax.lax.psum(x, axis_name=\"pmap_axis\")\n",
        "\n",
        "def pmap_f(x):\n",
        "  # We call all-gather on all devices. The\n",
        "  # leading device dimension will have the same\n",
        "  # value on each position.\n",
        "  return jax.pmap(f, axis_name=\"pmap_axis\")(x)[0]\n",
        "\n",
        "inputs = np.array([.1] * N_DEVICES)\n",
        "outs, jax_grads = jax.value_and_grad(pmap_f)(inputs)\n",
        "\n",
        "np.testing.assert_allclose(outs, inputs * N_DEVICES)\n",
        "np.testing.assert_allclose(jax_grads, [1.] * N_DEVICES)"
      ],
      "metadata": {
        "id": "8KO1PYkhJdBW",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:50:12.556746Z",
          "iopub.execute_input": "2023-07-12T15:50:12.557025Z",
          "iopub.status.idle": "2023-07-12T15:50:12.859944Z",
          "shell.execute_reply.started": "2023-07-12T15:50:12.557000Z",
          "shell.execute_reply": "2023-07-12T15:50:12.858536Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Case 2: pmap outside of gradient.\n",
        "def fwd(x):\n",
        "  # Just call all-gather from all devices.\n",
        "  return jax.lax.psum(x, axis_name='i')\n",
        "\n",
        "def fwd_bwd(x):\n",
        "  # Run forward pass, return gradient.\n",
        "  return jax.value_and_grad(fwd)(x)"
      ],
      "metadata": {
        "id": "VjLRz0DvQ7x4",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:50:12.861526Z",
          "iopub.execute_input": "2023-07-12T15:50:12.861864Z",
          "iopub.status.idle": "2023-07-12T15:50:12.867411Z",
          "shell.execute_reply.started": "2023-07-12T15:50:12.861835Z",
          "shell.execute_reply": "2023-07-12T15:50:12.866428Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_DEVICES = jax.local_device_count()\n",
        "\n",
        "input = jax.lax.broadcast(0.5 , (N_DEVICES,))\n",
        "val, grad = jax.pmap(fwd_bwd, axis_name='i')(input)\n",
        "# What do you expect gradient to be?"
      ],
      "metadata": {
        "id": "91lExBu6_uM1",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:50:12.868587Z",
          "iopub.execute_input": "2023-07-12T15:50:12.868851Z",
          "iopub.status.idle": "2023-07-12T15:50:13.016525Z",
          "shell.execute_reply.started": "2023-07-12T15:50:12.868828Z",
          "shell.execute_reply": "2023-07-12T15:50:13.015150Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val, grad"
      ],
      "metadata": {
        "id": "XzxJCsxJN_dg",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:50:13.018039Z",
          "iopub.execute_input": "2023-07-12T15:50:13.018347Z",
          "iopub.status.idle": "2023-07-12T15:50:13.028992Z",
          "shell.execute_reply.started": "2023-07-12T15:50:13.018321Z",
          "shell.execute_reply": "2023-07-12T15:50:13.027817Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Why is this the case?\n",
        "print(jax.make_jaxpr(jax.pmap(fwd_bwd, axis_name='i'))(input).pretty_print(use_color=True, source_info=True))"
      ],
      "metadata": {
        "id": "Pi5gkBh4NkoO",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:50:13.030239Z",
          "iopub.execute_input": "2023-07-12T15:50:13.030557Z",
          "iopub.status.idle": "2023-07-12T15:50:13.044603Z",
          "shell.execute_reply.started": "2023-07-12T15:50:13.030529Z",
          "shell.execute_reply": "2023-07-12T15:50:13.043501Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom VJPs"
      ],
      "metadata": {
        "id": "3l0kRCzZ7uCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `psum` forwards, `id` backwards."
      ],
      "metadata": {
        "id": "ZQM9zGPL7zUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since, we know what the problem is (an additional psum on the backward pass)\n",
        "# we need to implement a custom VJP that is psum on the forward pass, and\n",
        "# identity function during backward pass.\n",
        "\n",
        "# The process to implement a custom VJP has three steps:\n",
        "\n",
        "# Step 1: Define the function.\n",
        "@functools.partial(jax.custom_vjp, nondiff_argnums=(1, ))\n",
        "def fwd_psum_bwd_id(x, axis_name: str):\n",
        "    return jax.lax.psum(x, axis_name)\n",
        "\n",
        "# Step 2: Specify the forward pass, more specifically, the function\n",
        "# Should return the primal output and residuals (cached activations) that\n",
        "# we later need to calculate the VJP on the backward pass.\n",
        "def fwd_psum_bwd_id_fwd(\n",
        "    x: jnp.ndarray, axis_name: str) -> Tuple[jnp.ndarray, None]:\n",
        "  # Here, we need to calculate the psum on the forward pass.\n",
        "  # Since during the backward pass, we don't touch the incoming gradients,\n",
        "  # on only pass them through to earlier nodes in the computational graph\n",
        "  # we don't need to return any residuals.\n",
        "  return fwd_psum_bwd_id(x, axis_name), None\n",
        "\n",
        "# Step 3: Implement the backward pass.\n",
        "def fwd_psum_bwd_id_bwd(\n",
        "    unused_axis_name,\n",
        "    unused_residuals, g) -> Tuple[jnp.ndarray]:\n",
        "    # Pass through gradients. Note that we're returning a tuple.\n",
        "    return (g,)\n",
        "\n",
        "fwd_psum_bwd_id.defvjp(\n",
        "    fwd=fwd_psum_bwd_id_fwd,\n",
        "    bwd=fwd_psum_bwd_id_bwd)"
      ],
      "metadata": {
        "id": "rSLUf0T-XV0x",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:50:13.045869Z",
          "iopub.execute_input": "2023-07-12T15:50:13.046189Z",
          "iopub.status.idle": "2023-07-12T15:50:13.055082Z",
          "shell.execute_reply.started": "2023-07-12T15:50:13.046162Z",
          "shell.execute_reply": "2023-07-12T15:50:13.054022Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's see if the gradients match out expectations.\n",
        "def fwd(x):\n",
        "    return fwd_psum_bwd_id(x, axis_name='i')\n",
        "\n",
        "def fwd_bwd(x):\n",
        "    return jax.value_and_grad(fwd)(x)"
      ],
      "metadata": {
        "id": "ndo4z_mCamJA",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:50:13.056320Z",
          "iopub.execute_input": "2023-07-12T15:50:13.056725Z",
          "iopub.status.idle": "2023-07-12T15:50:13.068053Z",
          "shell.execute_reply.started": "2023-07-12T15:50:13.056699Z",
          "shell.execute_reply": "2023-07-12T15:50:13.067146Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val, grad = jax.pmap(fwd_bwd, axis_name='i')(input)"
      ],
      "metadata": {
        "id": "kMSBPD1Fa8sl",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:50:13.069245Z",
          "iopub.execute_input": "2023-07-12T15:50:13.069543Z",
          "iopub.status.idle": "2023-07-12T15:50:13.165122Z",
          "shell.execute_reply.started": "2023-07-12T15:50:13.069519Z",
          "shell.execute_reply": "2023-07-12T15:50:13.163911Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val, grad"
      ],
      "metadata": {
        "id": "CdEHWN9ta_kk",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:50:13.166504Z",
          "iopub.execute_input": "2023-07-12T15:50:13.166785Z",
          "iopub.status.idle": "2023-07-12T15:50:13.176762Z",
          "shell.execute_reply.started": "2023-07-12T15:50:13.166759Z",
          "shell.execute_reply": "2023-07-12T15:50:13.175624Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.make_jaxpr(jax.pmap(fwd_bwd, axis_name='i'))(input))"
      ],
      "metadata": {
        "id": "it-ZuCpU3cTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `id` forwards, `psum` backwards."
      ],
      "metadata": {
        "id": "D-G7boxT7_iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@functools.partial(jax.custom_vjp, nondiff_argnums=(1, ))\n",
        "def fwd_id_bwd_psum(x, axis_name: str):\n",
        "    ...\n",
        "\n",
        "\n",
        "def fwd_id_bwd_psum_fwd(x, axis_name) -> Tuple[jnp.ndarray, None]:\n",
        "    ...\n",
        "\n",
        "\n",
        "def fwd_id_bwd_psum_bwd(axis_name, unused_residuals, g):\n",
        "    ...\n",
        "\n",
        "\n",
        "fwd_id_bwd_psum.defvjp(\n",
        "    fwd=fwd_id_bwd_psum_fwd,\n",
        "    bwd=fwd_id_bwd_psum_bwd)"
      ],
      "metadata": {
        "id": "fdnGrtN7dkOW",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:50:13.184028Z",
          "iopub.execute_input": "2023-07-12T15:50:13.184746Z",
          "iopub.status.idle": "2023-07-12T15:50:13.191448Z",
          "shell.execute_reply.started": "2023-07-12T15:50:13.184712Z",
          "shell.execute_reply": "2023-07-12T15:50:13.190488Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's see if the gradients match out expectations.\n",
        "def fwd(x):\n",
        "    return fwd_id_bwd_psum(x, axis_name='i')\n",
        "\n",
        "def fwd_bwd(x):\n",
        "    return jax.value_and_grad(fwd)(x)"
      ],
      "metadata": {
        "id": "2muMBkX_8qXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jax.pmap(fwd_bwd, axis_name='i')(inputs)"
      ],
      "metadata": {
        "id": "6C-h-Qh-8vtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full example"
      ],
      "metadata": {
        "id": "zGwi4vTijkbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp_with_custom_vjps(params: Params, x: jax.Array) -> jax.Array:\n",
        "    \"\"\"Vanila MLP without dropout.\"\"\"\n",
        "    a, b = params\n",
        "\n",
        "    ...  # Id forwards, All-reduce backwards.\n",
        "    y = jnp.maximum(jnp.matmul(x, a), 0.0)\n",
        "    z = jnp.matmul(y, b)\n",
        "    ...   # All-reduce forwards, id backwards.\n",
        "    return z\n",
        "\n",
        "def loss_from_mlp(\n",
        "    params: Params, xs: jax.Array, ys: jax.Array):\n",
        "    return loss_fn(params, xs, ys, mlp_with_custom_vjps)\n",
        "\n",
        "\n",
        "@functools.partial(jax.pmap, axis_name='pmap_axis')\n",
        "def update_pmap_outside(params: Params, xs: jax.Array, ys: jax.Array):\n",
        "    loss_val, grads = jax.value_and_grad(loss_from_mlp)(params, xs, ys)\n",
        "    new_params = jax.tree_map(\n",
        "      lambda param, g: param - g * 0.01, params, grads)\n",
        "\n",
        "    return loss_val, grads, new_params"
      ],
      "metadata": {
        "id": "VFf46JVviccP",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:50:13.192597Z",
          "iopub.execute_input": "2023-07-12T15:50:13.192868Z",
          "iopub.status.idle": "2023-07-12T15:50:13.204388Z",
          "shell.execute_reply.started": "2023-07-12T15:50:13.192844Z",
          "shell.execute_reply": "2023-07-12T15:50:13.203426Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, params = init_fn(key, (num_shards, x_dim, out_dim))\n",
        "\n",
        "# Warmstart pmapped function and print statistics.\n",
        "loss_val, grads, _ =  update_pmap_outside(params, x_b, y_b)\n",
        "\n",
        "print(loss_val.shape)\n",
        "print(loss_val[0])\n",
        "\n",
        "dw1, dw2 = grads\n",
        "print(f\"Norm of dW_1 %.2f\" % np.linalg.norm(dw1))\n",
        "print(f\"Norm of dW_2 %.2f\\n\" % np.linalg.norm(dw2))"
      ],
      "metadata": {
        "id": "JOviuYuajfLU",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:50:13.205816Z",
          "iopub.execute_input": "2023-07-12T15:50:13.206116Z",
          "iopub.status.idle": "2023-07-12T15:50:13.706804Z",
          "shell.execute_reply.started": "2023-07-12T15:50:13.206090Z",
          "shell.execute_reply": "2023-07-12T15:50:13.705450Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit _, params = init_fn(key, (num_shards, x_dim, out_dim)); update_pmap_outside(params, x_b, y_b)\n",
        "loss_val, grads, params = update_pmap_outside(params, x_b, y_b)\n",
        "loss_val = loss_val[0]"
      ],
      "metadata": {
        "id": "_58GPyMsrKJg",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:50:13.708223Z",
          "iopub.execute_input": "2023-07-12T15:50:13.708578Z",
          "iopub.status.idle": "2023-07-12T15:50:18.779810Z",
          "shell.execute_reply.started": "2023-07-12T15:50:13.708548Z",
          "shell.execute_reply": "2023-07-12T15:50:18.778485Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Excercises for the reader"
      ],
      "metadata": {
        "id": "_BSIKVG0t51M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST\n",
        "\n",
        "* Implement the training loop with real data.\n",
        "* Compare single device mlp with sharded mlp (wrong gradients) and sharded mlp with properly implemented backwards pass."
      ],
      "metadata": {
        "id": "JjM40VRdt-4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "data_dir = '/tmp/tfds'\n",
        "\n",
        "# Fetch full datasets for evaluation\n",
        "# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
        "# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\n",
        "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)\n",
        "mnist_data = tfds.as_numpy(mnist_data)\n",
        "train_data, test_data = mnist_data['train'], mnist_data['test']\n",
        "num_labels = info.features['label'].num_classes\n",
        "h, w, c = info.features['image'].shape\n",
        "num_pixels = h * w * c\n",
        "\n",
        "# Full train set\n",
        "train_images, train_labels = train_data['image'], train_data['label']\n",
        "train_images = np.reshape(train_images, (len(train_images), num_pixels))\n",
        "train_labels = jax.nn.one_hot(train_labels, num_labels)\n",
        "\n",
        "# Full test set\n",
        "test_images, test_labels = test_data['image'], test_data['label']\n",
        "test_images = np.reshape(test_images, (len(test_images), num_pixels))\n",
        "test_labels = jax.nn.one_hot(test_labels, num_labels)\n",
        "\n",
        "\n",
        "print('Train:', train_images.shape, train_labels.shape)\n",
        "print('Test:', test_images.shape, test_labels.shape)"
      ],
      "metadata": {
        "id": "HNxLr9tEt7-d",
        "execution": {
          "iopub.status.busy": "2023-07-12T15:50:18.781282Z",
          "iopub.execute_input": "2023-07-12T15:50:18.781914Z",
          "iopub.status.idle": "2023-07-12T15:50:35.174882Z",
          "shell.execute_reply.started": "2023-07-12T15:50:18.781880Z",
          "shell.execute_reply": "2023-07-12T15:50:35.173513Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take home challenges.\n",
        "\n",
        "* Implement sharded Self-Attention from [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf).\n",
        "* Try adding sharded biases (and/or dropout). How would you handle random keys?\n",
        "* Experiments with GLU Variants: [GLU Variants Improve Transformer](https://arxiv.org/pdf/2002.05202.pdf)"
      ],
      "metadata": {
        "id": "OqBJZGSdt8gB"
      }
    }
  ]
}